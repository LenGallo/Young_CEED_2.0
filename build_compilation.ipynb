{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e370421-d2f7-4759-908a-b47e1c487b56",
   "metadata": {},
   "source": [
    "# build_compilation notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54acb53-153e-4623-be27-a73a37001046",
   "metadata": {},
   "source": [
    "This notebook iterates through each paleomagnetic record (datasheet) of the vgp database, extracts data which meet user-specified criteria, and appends them to a new dataframe for later processing (to generate an APWP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2daf731b-250b-4616-8cff-9e29f1d27413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pmagpy import ipmag, pmag\n",
    "from scripts.auxiliar import get_files_in_directory, spherical2cartesian, GCD_cartesian\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00c203d-b834-475e-8c2e-5ea52c06ffff",
   "metadata": {},
   "source": [
    "Set the directory from which we will pull the datasheets. These should be available as .csv files (one for each record according to the vgp template)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca35b575-9108-4b9f-b671-a4fece3c27f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_vgp = os.getcwd() + '/vgp_database'\n",
    "files_names = get_files_in_directory(data_path_vgp)\n",
    "csv_file_names = [os.path.splitext(os.path.basename(open(file,'r').name))[0] for file in files_names if file.endswith('.csv')] #consider just *csv files\n",
    "paths = [file for file in files_names if file.endswith('.csv')] \n",
    "files = pd.DataFrame({'path': paths, 'name': csv_file_names})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c691e5a0-6fd3-4489-aa61-278f543fd1e7",
   "metadata": {},
   "source": [
    "## Data selection criteria\n",
    "Specify the specific inclusion criteria to be used in the data-selection. If author_selection is set=1, all other criteria will be ignored. Setting values other than 'None' for the remaining criteria allow entries that were rejected by the original authors to be retrieved. This also allows selection criteria between studies to be easily homogenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "064a4b1a-f1d8-4b8d-a2a8-24f1ccb6347f",
   "metadata": {},
   "outputs": [],
   "source": [
    "incl_criteria = {\n",
    "    'author_selection': 1,     # 1 (yes) or 0 (no); if 1, all other criteria will be ignored\n",
    "    'undemagnetized': None,    # None (defaults to author selection) or 'y'  \n",
    "    'sample_count': None,      # None (defaults to author selection) or int: cutoff n (≥ x)\n",
    "    'alpha_95': None,          # None (defaults to author selection) or float: cutoff A95 (≤ x degrees)\n",
    "    'overprints': None,        # None (defaults to author selection) or 'y'  \n",
    "    'remagnetizations': None,  # None (defaults to author selection) or 'y'\n",
    "    'uncertain_struct': None,  # None (defaults to author selection) or 'y'\n",
    "    'rotated': None,           # None (defaults to author selection) or 'y'\n",
    "    'shallowed': None,         # None (defaults to author selection) or 'y' [***can also implement cutoff f-value here if desired***]\n",
    "    'anomalous_dir': None,     # None (defaults to author selection) or float: cutoff distance (in degrees) between vgp and mean (≤ x degrees)\n",
    "    'uncertain_age': None,     # None (defaults to author selection) or float: cutoff age resolution (in Myr) between min and max (≤ x Myr)\n",
    "    'distinct_age': None,      # None (defaults to author selection) or 'y'\n",
    "    'sub-time_units': None,    # None (defaults to author selection) or 'y'\n",
    "    'rock_type': None,         # None (defaults to author selection) or string: 'all' or 'igneous' or 'sedimentary'\n",
    "    'otherwise_rej': None,     # None (defaults to author selection) or 'y'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6707c0ea-85c5-4b2d-bd9b-d58397e8452a",
   "metadata": {},
   "source": [
    "Check the selection criteria and convert to numeric codes as used in the vgp database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbf9e54d-5acf-4074-86b0-a4f0c0decacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the numeric codes selected:  []\n"
     ]
    }
   ],
   "source": [
    "criteria_codes = []\n",
    "if incl_criteria['author_selection'] == 1: pass #ignore all other criteria if original selection is to be used\n",
    "elif incl_criteria['author_selection'] == 0:\n",
    "    if incl_criteria['undemagnetized'] == 'y': criteria_codes.append(1)\n",
    "    if incl_criteria['sample_count'] == type(int): criteria_codes.append(2)\n",
    "    if incl_criteria['alpha_95'] == type(float) or incl_criteria['alpha_95'] == type(int): criteria_codes.append(3)\n",
    "    if incl_criteria['overprints'] == 'y': criteria_codes.append(4)\n",
    "    if incl_criteria['remagnetizations'] == 'y': criteria_codes.append(5)\n",
    "    if incl_criteria['uncertain_struct'] == 'y': criteria_codes.append(6)\n",
    "    if incl_criteria['rotated'] == 'y': criteria_codes.append(7)\n",
    "    if incl_criteria['shallowed'] == 'y': criteria_codes.append(8)\n",
    "    if incl_criteria['anomalous_dir'] == type(float) or incl_criteria['anomalous_dir'] == type(int): criteria_codes.append(9)\n",
    "    if incl_criteria['uncertain_age'] == type(float) or incl_criteria['uncertain_age'] == type(int): criteria_codes.append(10)    \n",
    "    if incl_criteria['distinct_age'] == 'y': criteria_codes.append(11)\n",
    "    if incl_criteria['sub-time_units'] == 'y': criteria_codes.append(12)\n",
    "    if incl_criteria['rock_type'] == 'all' or incl_criteria['rock_type'] == 'igneous' or incl_criteria['rock_type'] == 'sedimentary': criteria_codes.append(13)\n",
    "    if incl_criteria['otherwise_rej'] == 'y': criteria_codes.append(14)\n",
    "else:\n",
    "    print ('invalid inclusion criterion selected for author_selection')\n",
    "    \n",
    "print ('the numeric codes selected: ', criteria_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed91afb6-01f3-46dd-a020-1d489cba62e6",
   "metadata": {},
   "source": [
    "## Initialize objects\n",
    "We seek to append all the data extracted from each datasheet into one place, we therefore initialize a master dataframe for vgps. We will also recalculate the study-level paleopoles based on our revised vgp selections, and so we also initialize a master dataframe for these recalculated poles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de78e1d5-b262-4c80-9e29-1cec56651aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vgps_master = pd.DataFrame()\n",
    "df_recalc_pps_master = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1507e518-2207-45a1-be31-192175d4290e",
   "metadata": {},
   "source": [
    "Several elements of the vgp database involve references between sites of a given datasheet: namely, stratigraphic ordering and common (redundant) instantaneous records of the field (e.g. 2 or more sites from the same cooling unit). This referencing is achieved by way of simple numeric codes which are repeated across datasheets. In order to preserve these references when we merge datasheets, we need to ensure these codes are made unique. This is easily achieved with use of counters, which we initialize at 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60383ab0-f5f2-400a-8fcb-157399810d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_group_counter = 0\n",
    "time_unit_counter = 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b179e747-666f-4dfe-9867-724cd9bd524a",
   "metadata": {},
   "source": [
    "## Example\n",
    "In order to demonstrate the processes executed below, we select an arbitrary single datasheet to process, before executing the same process on the entire database below. We first select a datasheet from among those in the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a5ddee7-5f11-4f0f-8834-3034f8aade70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    name\n",
       "0   test\n",
       "1  test2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[['name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82f961bc-397a-423d-beb2-e10d1e3df471",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_idx = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341a877e-b588-459f-ae03-3356f65b1a78",
   "metadata": {},
   "source": [
    "### Split study- and site-level data\n",
    "Each datasheet contains both study-level poles and site-level vgps. We split these data and assign them to separate dataframes. We can also cast types for specific columns to avoid problems later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bef8f1c3-4d34-4957-aa89-41949a5bb613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_datasheet (files, file_idx): \n",
    "    df = pd.read_csv(files['path'][file_idx], skip_blank_lines = False, encoding = \"ISO-8859-1\") #, skip_blank_lines=True\n",
    "    df_list = np.split(df, df[df.isnull().all(1)].index)\n",
    "    df_poles = df_list[0]\n",
    "    df_vgps = df_list[1].dropna(how='all')\n",
    "    df_vgps =df_vgps.rename(columns = df_vgps.iloc[0]).drop(df_vgps.index[0]) # assign the first row as columns for the df_vgps\n",
    "\n",
    "    #cast columns\n",
    "    df_poles = df_poles.astype({'pole': int, 'N': int,\n",
    "                              \"slat\":float, \"slon\":float, \"dec\":float, \"inc\":float,\n",
    "                              \"Plat\":float, \"Plon\":float})\n",
    "    df_vgps = df_vgps.astype({'in_study_pole': int, 'strat_group': int, 'n': int, 'k': float,\n",
    "                              \"slat\":float, \"slon\":float, \"dec\":float, \"inc\":float, 'alpha95': float, 'time_unit': str,\n",
    "                             'min_age':float, 'max_age': float, 'mean_age':float,\n",
    "                              \"VGP_lat\":float, \"VGP_lon\":float})\n",
    "    return (df_poles, df_vgps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89b793aa-c9f0-4313-959a-cd036da96666",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poles, df_vgps = split_datasheet(files, file_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eabcf73-742f-463d-a53d-dacbd57bc85d",
   "metadata": {},
   "source": [
    "Have a look at these dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57a6157f-d1a1-4dc6-9aff-3f27543c2896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pole</th>\n",
       "      <th>name</th>\n",
       "      <th>slat</th>\n",
       "      <th>slon</th>\n",
       "      <th>N</th>\n",
       "      <th>dec</th>\n",
       "      <th>inc</th>\n",
       "      <th>k</th>\n",
       "      <th>alpha95</th>\n",
       "      <th>f_corr</th>\n",
       "      <th>Plat</th>\n",
       "      <th>Plon</th>\n",
       "      <th>K</th>\n",
       "      <th>A95</th>\n",
       "      <th>dp</th>\n",
       "      <th>dm</th>\n",
       "      <th>mean_age</th>\n",
       "      <th>2sig_mean</th>\n",
       "      <th>min_age</th>\n",
       "      <th>2sig_min</th>\n",
       "      <th>max_age</th>\n",
       "      <th>2sig_max</th>\n",
       "      <th>error_dist</th>\n",
       "      <th>lithology_1</th>\n",
       "      <th>lithology_2</th>\n",
       "      <th>R1</th>\n",
       "      <th>R2</th>\n",
       "      <th>R3</th>\n",
       "      <th>R4</th>\n",
       "      <th>R5</th>\n",
       "      <th>R6</th>\n",
       "      <th>R7</th>\n",
       "      <th>pmag_ref</th>\n",
       "      <th>age_ref</th>\n",
       "      <th>pmag_comments</th>\n",
       "      <th>age_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Quaternary age rocks, Western Central TMVB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>2.9</td>\n",
       "      <td>37.5</td>\n",
       "      <td>38</td>\n",
       "      <td>7.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86.7</td>\n",
       "      <td>314.0</td>\n",
       "      <td>43</td>\n",
       "      <td>7.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>uniform</td>\n",
       "      <td>igneous; volcanic</td>\n",
       "      <td>mafic to intermediate lavas</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Ruiz-Martínez et al. (2010)</td>\n",
       "      <td>GTS2020</td>\n",
       "      <td>no field stability tests; structural coherence...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>late Miocene-Pliocene age rocks, Western Centr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33</td>\n",
       "      <td>0.6</td>\n",
       "      <td>38.2</td>\n",
       "      <td>22</td>\n",
       "      <td>5.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88.0</td>\n",
       "      <td>265.5</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>uniform</td>\n",
       "      <td>igneous; volcanic</td>\n",
       "      <td>mafic to intermediate lavas</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Ruiz-Martínez et al. (2010)</td>\n",
       "      <td>GTS2020</td>\n",
       "      <td>no field stability tests; structural coherence...</td>\n",
       "      <td>min age is Pliocene-Quaternary boundary; max a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pole                                               name  slat  slon   N  \\\n",
       "0     1         Quaternary age rocks, Western Central TMVB   NaN   NaN  10   \n",
       "1     2  late Miocene-Pliocene age rocks, Western Centr...   NaN   NaN  33   \n",
       "\n",
       "   dec   inc   k alpha95 f_corr  Plat   Plon   K  A95   dp   dm mean_age  \\\n",
       "0  2.9  37.5  38     7.9    NaN  86.7  314.0  43  7.5  NaN  NaN      NaN   \n",
       "1  0.6  38.2  22     5.4    NaN  88.0  265.5  26    5  NaN  NaN      NaN   \n",
       "\n",
       "  2sig_mean min_age 2sig_min max_age 2sig_max error_dist        lithology_1  \\\n",
       "0       NaN       0      NaN    2.58      NaN    uniform  igneous; volcanic   \n",
       "1       NaN    2.58      NaN    11.2      NaN    uniform  igneous; volcanic   \n",
       "\n",
       "                   lithology_2 R1 R2 R3 R4   R5 R6 R7  \\\n",
       "0  mafic to intermediate lavas  1  1  1  0  0.5  1  1   \n",
       "1  mafic to intermediate lavas  1  1  1  0  0.5  1  1   \n",
       "\n",
       "                      pmag_ref  age_ref  \\\n",
       "0  Ruiz-Martínez et al. (2010)  GTS2020   \n",
       "1  Ruiz-Martínez et al. (2010)  GTS2020   \n",
       "\n",
       "                                       pmag_comments  \\\n",
       "0  no field stability tests; structural coherence...   \n",
       "1  no field stability tests; structural coherence...   \n",
       "\n",
       "                                        age_comments  \n",
       "0                                                NaN  \n",
       "1  min age is Pliocene-Quaternary boundary; max a...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_poles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "078f7386-472d-4121-a5ee-bc511c054287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>fm./loc.</th>\n",
       "      <th>slat</th>\n",
       "      <th>slon</th>\n",
       "      <th>n</th>\n",
       "      <th>dec</th>\n",
       "      <th>inc</th>\n",
       "      <th>k</th>\n",
       "      <th>alpha95</th>\n",
       "      <th>f_corr</th>\n",
       "      <th>VGP_lat</th>\n",
       "      <th>VGP_lon</th>\n",
       "      <th>K</th>\n",
       "      <th>A95</th>\n",
       "      <th>dp</th>\n",
       "      <th>dm</th>\n",
       "      <th>mean_age</th>\n",
       "      <th>2sig_mean</th>\n",
       "      <th>min_age</th>\n",
       "      <th>2sig_min</th>\n",
       "      <th>max_age</th>\n",
       "      <th>2sig_max</th>\n",
       "      <th>error_dist</th>\n",
       "      <th>lithology_1</th>\n",
       "      <th>lithology_2</th>\n",
       "      <th>polarity</th>\n",
       "      <th>strat_group</th>\n",
       "      <th>ordering</th>\n",
       "      <th>level</th>\n",
       "      <th>time_unit</th>\n",
       "      <th>in_study_pole</th>\n",
       "      <th>rej_crit</th>\n",
       "      <th>pmag_ref</th>\n",
       "      <th>age_ref</th>\n",
       "      <th>pmag_comments</th>\n",
       "      <th>age_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mwnHIG</td>\n",
       "      <td>Western sector of TMVB</td>\n",
       "      <td>20.79</td>\n",
       "      <td>-105.48</td>\n",
       "      <td>11</td>\n",
       "      <td>343.6</td>\n",
       "      <td>24.6</td>\n",
       "      <td>37.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.4</td>\n",
       "      <td>140.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.20</td>\n",
       "      <td>0.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>normal</td>\n",
       "      <td>igneous; volcanic</td>\n",
       "      <td>mafic to intermediate lavas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ruiz-Martínez et al. (2010)</td>\n",
       "      <td>Ruiz-Martínez et al. (2010)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mwrSF</td>\n",
       "      <td>Western sector of TMVB</td>\n",
       "      <td>20.89</td>\n",
       "      <td>-105.41</td>\n",
       "      <td>11</td>\n",
       "      <td>190.8</td>\n",
       "      <td>-45.3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>30.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>normal</td>\n",
       "      <td>igneous; volcanic</td>\n",
       "      <td>mafic to intermediate lavas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Ruiz-Martínez et al. (2010)</td>\n",
       "      <td>Ruiz-Martínez et al. (2010)</td>\n",
       "      <td>scattered directions</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pwnPLA</td>\n",
       "      <td>Western sector of TMVB</td>\n",
       "      <td>21.35</td>\n",
       "      <td>-105.24</td>\n",
       "      <td>8</td>\n",
       "      <td>351.1</td>\n",
       "      <td>31.3</td>\n",
       "      <td>60.0</td>\n",
       "      <td>6.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.5</td>\n",
       "      <td>138.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>uniform</td>\n",
       "      <td>igneous; volcanic</td>\n",
       "      <td>mafic to intermediate lavas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ruiz-Martínez et al. (2010)</td>\n",
       "      <td>Ruiz-Martínez et al. (2010)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pwrLIB</td>\n",
       "      <td>Western sector of TMVB</td>\n",
       "      <td>21.58</td>\n",
       "      <td>-105.19</td>\n",
       "      <td>10</td>\n",
       "      <td>166.5</td>\n",
       "      <td>-41.1</td>\n",
       "      <td>492.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77.4</td>\n",
       "      <td>176.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>uniform</td>\n",
       "      <td>igneous; volcanic</td>\n",
       "      <td>mafic to intermediate lavas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ruiz-Martínez et al. (2010)</td>\n",
       "      <td>Ruiz-Martínez et al. (2010)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pwrJOL</td>\n",
       "      <td>Western sector of TMVB</td>\n",
       "      <td>21.40</td>\n",
       "      <td>-105.18</td>\n",
       "      <td>7</td>\n",
       "      <td>8.4</td>\n",
       "      <td>49.1</td>\n",
       "      <td>78.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78.6</td>\n",
       "      <td>294.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.36</td>\n",
       "      <td>0.17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>normal</td>\n",
       "      <td>igneous; volcanic</td>\n",
       "      <td>mafic to intermediate lavas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ruiz-Martínez et al. (2010)</td>\n",
       "      <td>Ruiz-Martínez et al. (2010)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name                fm./loc.   slat    slon   n    dec   inc      k  \\\n",
       "4  mwnHIG  Western sector of TMVB  20.79 -105.48  11  343.6  24.6   37.0   \n",
       "5   mwrSF  Western sector of TMVB  20.89 -105.41  11  190.8 -45.3    2.0   \n",
       "6  pwnPLA  Western sector of TMVB  21.35 -105.24   8  351.1  31.3   60.0   \n",
       "7  pwrLIB  Western sector of TMVB  21.58 -105.19  10  166.5 -41.1  492.0   \n",
       "8  pwrJOL  Western sector of TMVB  21.40 -105.18   7    8.4  49.1   78.0   \n",
       "\n",
       "   alpha95 f_corr  VGP_lat  VGP_lon    K  A95   dp   dm  mean_age 2sig_mean  \\\n",
       "4      6.9    NaN     72.4    140.4  NaN  NaN  NaN  NaN     10.20       0.8   \n",
       "5     30.5    NaN      NaN      NaN  NaN  NaN  NaN  NaN     11.10       0.2   \n",
       "6      6.4    NaN     80.5    138.4  NaN  NaN  NaN  NaN       NaN       NaN   \n",
       "7      2.2    NaN     77.4    176.4  NaN  NaN  NaN  NaN       NaN       NaN   \n",
       "8      6.9    NaN     78.6    294.4  NaN  NaN  NaN  NaN      3.36      0.17   \n",
       "\n",
       "   min_age 2sig_min  max_age 2sig_max error_dist        lithology_1  \\\n",
       "4      NaN      NaN      NaN      NaN     normal  igneous; volcanic   \n",
       "5      NaN      NaN      NaN      NaN     normal  igneous; volcanic   \n",
       "6     2.58      NaN      3.5      NaN    uniform  igneous; volcanic   \n",
       "7     2.58      NaN      3.5      NaN    uniform  igneous; volcanic   \n",
       "8      NaN      NaN      NaN      NaN     normal  igneous; volcanic   \n",
       "\n",
       "                   lithology_2 polarity  strat_group ordering level time_unit  \\\n",
       "4  mafic to intermediate lavas      NaN            0      NaN   NaN         0   \n",
       "5  mafic to intermediate lavas      NaN            0      NaN   NaN         0   \n",
       "6  mafic to intermediate lavas      NaN            0      NaN   NaN         0   \n",
       "7  mafic to intermediate lavas      NaN            0      NaN   NaN         0   \n",
       "8  mafic to intermediate lavas      NaN            0      NaN   NaN         2   \n",
       "\n",
       "   in_study_pole rej_crit                     pmag_ref  \\\n",
       "4              2      NaN  Ruiz-Martínez et al. (2010)   \n",
       "5              0        3  Ruiz-Martínez et al. (2010)   \n",
       "6              2      NaN  Ruiz-Martínez et al. (2010)   \n",
       "7              2      NaN  Ruiz-Martínez et al. (2010)   \n",
       "8              2      NaN  Ruiz-Martínez et al. (2010)   \n",
       "\n",
       "                       age_ref         pmag_comments age_comments  \n",
       "4  Ruiz-Martínez et al. (2010)                   NaN          NaN  \n",
       "5  Ruiz-Martínez et al. (2010)  scattered directions          NaN  \n",
       "6  Ruiz-Martínez et al. (2010)                   NaN          NaN  \n",
       "7  Ruiz-Martínez et al. (2010)                   NaN          NaN  \n",
       "8  Ruiz-Martínez et al. (2010)                   NaN          NaN  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vgps.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313cafad-4a9e-4059-9bdc-97eaf42a4f70",
   "metadata": {},
   "source": [
    "## Fill-in missing data\n",
    "The datasheets in the database are mostly only populated with data reported in the original publication, which means that some series will be empty. In some cases, we can estimate values for these missing series based on other reported data (e.g. computing a vgp from reported dec/inc and slat/slon values). In the following we seek to populate the specific series that we may need to utilize later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97af4ab6-c077-4b9f-9619-b2f264403b70",
   "metadata": {},
   "source": [
    "### Check / fill-in any missing vgps.\n",
    "(or otherwise drop entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "288286fe-724e-4c02-ba31-f3d1391bd87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vgps (df, file_idx): #seeks to fill in missing vgp entries in dataframe\n",
    "    \n",
    "    # first identify any entries missing vgp information\n",
    "    df['VGP_exists'] = df.apply(lambda row: True if not (np.isnan(row.VGP_lat) or np.isnan(row.VGP_lon)) else False, axis=1)\n",
    "    df_missing_vgps = df[df['VGP_exists'] == False]\n",
    "    \n",
    "    if df_missing_vgps.empty:\n",
    "        print ('no missing vgp information')\n",
    "    \n",
    "    else:\n",
    "        # now check that those which are missing vgp data have sufficient information to calculate it (dec/inc + site data)\n",
    "        df_missing_vgps['sufficient'] = df_missing_vgps.apply(lambda row: True if not (np.isnan(row.slat) or np.isnan(row.slon) or np.isnan(row.dec) or np.isnan(row.inc)) \\\n",
    "                                                    else False, axis=1)\n",
    "\n",
    "        # report any sites where critical information is lacking \n",
    "        if not df_missing_vgps['sufficient'].all():\n",
    "            missing_idx = df_missing_vgps.index[df_missing_vgps['sufficient'] == False].tolist()\n",
    "            for i in missing_idx:\n",
    "                site = df['name'][i]\n",
    "                print (f'Missing slat/slon and/or dec/inc at site {site} in file index {file_idx} where no vgp is reported; cannot calculate vgp -- dropping entry') \n",
    "\n",
    "            # drop entries with no vgp\n",
    "            df.drop(labels=missing_idx, inplace=True)\n",
    "                \n",
    "        # calculate vgps. This adds columns: 'paleolatitude', 'vgp_lat', 'vgp_lon', 'vgp_lat_rev' and 'vgp_lon_rev'\n",
    "        df_get_vgps = df_missing_vgps[df_missing_vgps['sufficient'] == True]\n",
    "        ipmag.vgp_calc(df_get_vgps, site_lon='slon', site_lat='slat', dec_tc='dec', inc_tc='inc')\n",
    "\n",
    "        # assign calculated vgps to original dataframe\n",
    "        df.VGP_lat.fillna(df_get_vgps.vgp_lat, inplace=True)\n",
    "        df.VGP_lon.fillna(df_get_vgps.vgp_lon, inplace=True)\n",
    "    \n",
    "    df.drop(['VGP_exists'], axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fc3ef9a-27b3-4809-9c25-291a44f9249a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing slat/slon and/or dec/inc at site pwrIXT in file index 1 where no vgp is reported; cannot calculate vgp -- dropping entry\n"
     ]
    }
   ],
   "source": [
    "df_vgps = get_vgps(df_vgps, file_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33b044d-b0c2-4406-8293-4c9a9cf15454",
   "metadata": {},
   "source": [
    "### Check / fill-in any missing site coordinate information.\n",
    "(or otherwise drop entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "215ae41b-6ff5-498e-b3a5-699392c20fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sites (df, file_idx): #seeks to fill in missing site coordinates in dataframe\n",
    "    \n",
    "    # first identify any entries missing site information\n",
    "    df['site_exists'] = df.apply(lambda row: True if not (np.isnan(row.slat) or np.isnan(row.slon)) else False, axis=1)\n",
    "    df_missing_sites = df[df['site_exists'] == False]\n",
    "    \n",
    "    if df_missing_sites.empty:\n",
    "        print ('no missing site information')\n",
    "    \n",
    "    else:\n",
    "        # now check that those which are missing site data have sufficient information to calculate it (dec/inc + vgp coordinates)\n",
    "        df_missing_sites['sufficient'] = df_missing_sites.apply(lambda row: True if not (np.isnan(row.slat) or np.isnan(row.slon) or np.isnan(row.dec) or np.isnan(row.inc)) \\\n",
    "                                                    else False, axis=1)\n",
    "\n",
    "        # report any sites where critical information is lacking \n",
    "        if not df_missing_sites['sufficient'].all():\n",
    "            missing_idx = df_missing_sites.index[df_missing_sites['sufficient'] == False].tolist()\n",
    "            for i in missing_idx:\n",
    "                location = df['name'][i]\n",
    "                print (f'Missing dec/inc and/or vgp at site {location} in file index {file_idx} where no site coordinates are reported; cannot calculate site coordinates -- dropping entry')\n",
    "\n",
    "        # calculate site coordinates. ### *** THIS FUNCTION REMAINS TO BE BUILT *** ###\n",
    "        df_get_site = df_missing_sites[df_missing_sites['sufficient'] == True]\n",
    "        ### FUNCTION TO BE BUILT\n",
    "\n",
    "        # assign calculated site coordinates to original dataframe\n",
    "        #df.slat.fillna(df_get_site.site_lat, inplace=True)\n",
    "        #df.slon.fillna(df_get_site.site_lon, inplace=True)\n",
    "\n",
    "        # drop entries with no vgp and the added column\n",
    "        df.drop(labels=missing_idx, inplace=True)\n",
    "    \n",
    "    df.drop(['site_exists'], axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30084642-f610-4681-9d0a-b87fde0a072b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no missing site information\n"
     ]
    }
   ],
   "source": [
    "df_vgps = get_sites(df_vgps, file_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae78eb0-9b3f-4206-94bb-1652c85dc30f",
   "metadata": {},
   "source": [
    "### Cross-check vgps against dec/inc and slat/slon\n",
    "Now that we have ensured all the key data are filled in, check that the vgps are self-consistent with the dec/inc and slat/slon data.\n",
    "Where poles appear to have been inverted, flip back to the correct polarity. Flag any otherwise spurious vgps to be checked against original reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54ef72da-cb8c-4551-ab1f-e213c58d29a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_vgps (df, file_idx): #check vgps against dec/inc and slat/slon data\n",
    "    \n",
    "    # compute vgp from dec/inc & slat/slon\n",
    "    ipmag.vgp_calc(df, site_lon='slon', site_lat='slat', dec_tc='dec', inc_tc='inc')\n",
    "    \n",
    "    # measure distance between recalculated vgp and listed vgp\n",
    "    df['GCD'] = df.apply(lambda row: pmag.angle([row.VGP_lon, row.VGP_lat], [row.vgp_lon, row.vgp_lat]), axis=1)\n",
    "    \n",
    "    # if angle is greater than 178 degrees, assume it was inverted by original authors and re-invert\n",
    "    invert_idx = df.index[df['GCD'] > 178.0].tolist()\n",
    "    for i in invert_idx:\n",
    "        location = df['name'][i]\n",
    "        print (f'vgp from site {location} in file index {file_idx} appears to be inverted. Flipping back.')\n",
    "    \n",
    "    df['VGP_lat'] = np.where(df['GCD'] > 178., -df['VGP_lat'], df['VGP_lat'])\n",
    "    df['VGP_lon'] = np.where(df['GCD'] > 178., (df['VGP_lon']-180.) % 360., df['VGP_lon'])\n",
    "    \n",
    "    # if any angle is between 2 and 178 degrees, flag it as spurious\n",
    "    spurious_idx = df.index[(df['GCD'] > 2.0) & (df['GCD'] < 178.0)].tolist()\n",
    "    for i in spurious_idx:\n",
    "        location = df['name'][i]\n",
    "        angle = int(df['GCD'][i])\n",
    "        print (f'***SPURIOUS*** vgp from site {location} in file index {file_idx}; reported pole differs from re-calculated by {angle} degrees. CHECK against original reference')\n",
    "        \n",
    "    # drop added columnS\n",
    "    df.drop(['GCD', 'paleolatitude', 'vgp_lat', 'vgp_lon', 'vgp_lat_rev', 'vgp_lon_rev'], axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "286f2061-1010-4787-8581-5faa01e41262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vgp from site pwrLIB in file index 1 appears to be inverted. Flipping back.\n",
      "vgp from site pwnPAL in file index 1 appears to be inverted. Flipping back.\n",
      "vgp from site pwrJAL in file index 1 appears to be inverted. Flipping back.\n",
      "vgp from site pwrSJG in file index 1 appears to be inverted. Flipping back.\n",
      "vgp from site pwrFER in file index 1 appears to be inverted. Flipping back.\n",
      "vgp from site mcrARE in file index 1 appears to be inverted. Flipping back.\n",
      "vgp from site pcrCHA in file index 1 appears to be inverted. Flipping back.\n",
      "vgp from site mcrFIN in file index 1 appears to be inverted. Flipping back.\n",
      "vgp from site pcrTRO in file index 1 appears to be inverted. Flipping back.\n",
      "vgp from site pcrOCO in file index 1 appears to be inverted. Flipping back.\n",
      "vgp from site qcrCG in file index 1 appears to be inverted. Flipping back.\n",
      "vgp from site pcrPEN in file index 1 appears to be inverted. Flipping back.\n",
      "vgp from site qcrEST in file index 1 appears to be inverted. Flipping back.\n",
      "vgp from site qcrROD in file index 1 appears to be inverted. Flipping back.\n",
      "vgp from site qcrCSA in file index 1 appears to be inverted. Flipping back.\n",
      "vgp from site pcrHUA in file index 1 appears to be inverted. Flipping back.\n",
      "vgp from site pcrSOL in file index 1 appears to be inverted. Flipping back.\n",
      "vgp from site mcrCAN in file index 1 appears to be inverted. Flipping back.\n",
      "vgp from site mcrORD in file index 1 appears to be inverted. Flipping back.\n",
      "vgp from site mcrAZU in file index 1 appears to be inverted. Flipping back.\n",
      "***SPURIOUS*** vgp from site pcrTRA in file index 1; reported pole differs from re-calculated by 173 degrees. CHECK against original reference\n"
     ]
    }
   ],
   "source": [
    "df_vgps = check_vgps (df_vgps, file_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fcc5fc-2f49-4a76-9823-50934148836b",
   "metadata": {},
   "source": [
    "### Establish common polarity convention\n",
    "Add an extra column where all vgps are cast into reverse polarity to make common calculations easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "305bb8b5-2138-40d4-ae52-d5bc4b70da90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_polarity (df): # cast all vgps into the southern hemisphere\n",
    "    \n",
    "    # *** NOTE: This function should be generalized so that we don't invert low-latitude reverse polarity poles the wrong way!\n",
    "    # this will be increasingly more important as we go further backward in time, as we cannot rely on the assumption that reverse\n",
    "    # polarity poles should be in southern hemisphere\n",
    "\n",
    "    df['VGP_lat_Rpol'] = np.where(df['VGP_lat'] > 0, -df['VGP_lat'], df['VGP_lat'])\n",
    "    df['VGP_lon_Rpol'] = np.where(df['VGP_lat'] > 0, (df['VGP_lon'] - 180.) % 360., df['VGP_lon'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cdce0c44-778d-485c-8772-b1425d094c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vgps = get_common_polarity(df_vgps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734cd550-3641-417a-9089-640feec367e7",
   "metadata": {},
   "source": [
    "### Check / fill-in any missing alpha 95's.\n",
    "(or otherwise set to 999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a83c0483-6c9e-45d1-b446-edaef3b00d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alpha95s (df, file_idx): #seeks to fill in missing alpha95s in dataframe\n",
    "    \n",
    "    # first identify any entries missing alpha95 information\n",
    "    df['a95_exists'] = df.apply(lambda row: True if not np.isnan(row.alpha95) else False, axis=1)\n",
    "    df_missing_a95s = df[df['a95_exists'] == False]\n",
    "    \n",
    "    if df_missing_a95s.empty:\n",
    "        print ('no missing alpha95 information')\n",
    "\n",
    "    else:\n",
    "        # now check that those which are missing alpha95 data have sufficient information to calculate it (n & k)\n",
    "        df_missing_a95s['sufficient'] = df_missing_a95s.apply(lambda row: True if not (np.isnan(row.n) or np.isnan(row.k)) \\\n",
    "                                                    else False, axis=1)\n",
    "\n",
    "        # report any sites where critical information is lacking \n",
    "        if not df_missing_a95s['sufficient'].all():\n",
    "            missing_idx = df_missing_a95s.index[df_missing_a95s['sufficient'] == False].tolist()\n",
    "            for i in missing_idx:\n",
    "                location = df['name'][i]\n",
    "                print (f'Missing n and/or k at site {location} in file index {file_idx} where no alpha95 is reported; cannot calculate alpha95 -- setting to 999')\n",
    "\n",
    "        # calculate site coordinates. ### *** THIS FUNCTION REMAINS TO BE BUILT *** ###\n",
    "        df_get_a95s = df_missing_a95s[df_missing_a95s['sufficient'] == True]\n",
    "        df_get_a95s['a95'] = df_get_a95s.apply(lambda row: 140.0/np.sqrt(row.n * row.k), axis=1)\n",
    "\n",
    "        # assign calculated a95s to original dataframe, set those which could not be calculated to 999\n",
    "        df.alpha95.fillna(df_get_a95s.a95, inplace=True)\n",
    "        \n",
    "        # set those which could not be calculated to 999, and drop added column\n",
    "        df.alpha95.fillna(value=999)\n",
    "    \n",
    "    df.drop(['a95_exists'], axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "326b86b1-b691-45a7-80e6-3989cbbb30b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no missing alpha95 information\n"
     ]
    }
   ],
   "source": [
    "df_vgps = get_alpha95s(df_vgps, file_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad205637-e7ad-4213-925d-8f16075dd6b9",
   "metadata": {},
   "source": [
    "#### TO DO: fill in min and max ages for normally distributed ages\n",
    "<em> OR SHOULD WE JUST SIMPLIFY DATASHEETS TO ONLY INCLUDE MEAN AGE, MIN_AGE, MAX_AGE & ERROR_TYPE? </em>  \n",
    "... for time being have just made function to ensure mean ages are filled in ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b6dd1df-9bab-4d9f-916c-72afc4a0086d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ages (df):\n",
    "    \n",
    "    df['mean_age'] = df.apply(lambda row: (row.min_age + row.max_age)/2.0 if np.isnan(row.mean_age) else row.mean_age, axis=1)\n",
    "   \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1c5f4a9-6464-4222-b9dd-be1b1b5caa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vgps = get_ages(df_vgps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f5442b-e449-482c-a2a8-6db4714f9f1d",
   "metadata": {},
   "source": [
    "## Assign unique IDs to strat_groups and time_units\n",
    "Change the numeric codes assigned to strat_group and time_unit entries according to counters so that they are uniquely identifiable after merger into the central dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f59ae546-82d3-4726-9e0c-c2ec1cd3b411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_uniq_ids (df, strat_group_counter, time_unit_counter):  #assigns new codes to local strat_groups and time_units to make unique across entire database\n",
    "    \n",
    "    # assign new codes to strat_groups\n",
    "    df['strat_group'] = df.apply(lambda row: row.strat_group + strat_group_counter if not row.strat_group == 0 else row.strat_group, axis=1)\n",
    "\n",
    "    #update strat_group_counter with new max value from local list\n",
    "    strat_group_counter = df['strat_group'].max()\n",
    "\n",
    "    # update time_unit identifiers (note that some entries have an 'M' prefix that designates them as a local mean)\n",
    "    df.time_unit.fillna(value='0')\n",
    "    df.time_unit = df.time_unit.astype('str') # ensure that time_unit entries are strings\n",
    "    df['time_unit'] = df.apply(lambda row: ' '.join(re.findall(\"[a-zA-Z]+\", row.time_unit)) + str(int(''.join(filter(str.isdigit, row.time_unit))) \\\n",
    "                                                    + time_unit_counter) if not row.time_unit == '0' else row.time_unit, axis = 1)\n",
    "    \n",
    "    #update time_unit_counter with new max value\n",
    "    time_unit_counter = pd.to_numeric(df['time_unit'], 'coerce').max()\n",
    "\n",
    "    return (df, strat_group_counter, time_unit_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "242e5a34-0684-4eac-9633-6da0198e12f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vgps, strat_group_counter, time_unit_counter = assign_uniq_ids(df_vgps, strat_group_counter, time_unit_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fc692a-0cff-486d-b6ba-cd84f5b2b2ea",
   "metadata": {},
   "source": [
    "Because we will loop over the entire database below (including this datasheet), we will here reset the counter back to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e33f985-2c44-4eaf-baa1-052974aabdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_group_counter = 0\n",
    "time_unit_counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898c2270-7bfe-4cb5-9107-c71082862571",
   "metadata": {},
   "source": [
    "## Filter data\n",
    "Now we evaluate the entries against the specified inclusion criteria. We start with an 'initial' filter, which removes any entries that don't have the right basic inclusion codes and those which fail any specified n, alpha95, age uncertainty and/or rock type criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9f0b8410-c908-48b1-a954-a66929fd924c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_filter (df, incl_criteria, criteria_codes): #flag the subset of entries which fulfill the specified inclusion criteria\n",
    "    \n",
    "    df['rej_crit'] = df['rej_crit'].fillna(0) # replace NaNs in this column with 0's\n",
    "    \n",
    "    # make a new temporary series that first flags those entries which do / don't pass the basic inclusion criteria according to rej_crit codes\n",
    "    df['keep'] = df.apply(lambda row: True if row.in_study_pole != 0 or all(crit in criteria_codes for crit in [int(i) for i in str(row.rej_crit).split(',')]) \\\n",
    "                                else False, axis=1)\n",
    "    \n",
    "    # reject any entries with too small sample count\n",
    "    if 2 in criteria_codes:\n",
    "        df['keep'] = df.apply(lambda row: False if row.n < incl_criteria['sample_count'] else row.keep, axis=1)\n",
    "\n",
    "    # reject any entries with too large alpha 95\n",
    "    if 3 in criteria_codes:  \n",
    "        df['keep'] = df.apply(lambda row: False if row.alpha95 > incl_criteria['alpha_95'] else row.keep, axis=1)\n",
    "\n",
    "    # reject vgps with too large age uncertainty (as determined by diff b/w min and max)\n",
    "    if 10 in criteria_codes:\n",
    "        df['keep'] = df.apply(lambda row: False if (row.max_age - row.min_age) > incl_criteria['uncertain_age'] else row.keep, axis=1)\n",
    "\n",
    "    # reject vgps with wrong rock type\n",
    "    if 13 in criteria_codes: \n",
    "        if incl_criteria['rock_type'] == 'all': pass\n",
    "        else: df['keep'] = df.apply(lambda row: False if row.rock_type != incl_criteria['rock_type'] else row.keep, axis=1)\n",
    "            \n",
    "    df.drop(df[df.keep == False].index, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80055c0e-66cf-4610-aa27-e8ac5ab51dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vgps = init_filter(df_vgps, incl_criteria, criteria_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c3e788-0dcf-4f41-ae95-a877431a43d2",
   "metadata": {},
   "source": [
    "We still need to find and reject any anomalous vgps, if any such criteria were specified above. However, in order to evaluate this, we need a provisional paleopole. Before we compute that we need to remove any vgps with distinct ages (since they shouldn't contribute to the paleopole calculation). These temporally distinctive entries can be sent to the master vgp dataframe and removed from our local selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e7a91f9-ea74-4e02-b6a4-5f683fb5c682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_age_distinct (df, df_master, criteria_codes):\n",
    "\n",
    "    # check if any entries with distinct age and flag with a NaN in the 'keep' column\n",
    "    if 11 in criteria_codes:\n",
    "        df['keep'] = df.apply(lambda row: np.nan if (row.keep == True and 11 in [int(i) for i in str(row.rej_crit).split(',')]) else row.keep, axis=1) \n",
    "\n",
    "        #pass these distinct age vgps to the master vgp dataframe\n",
    "        df_distinct = df[df['keep'] == np.nan]\n",
    "        df_master = pd.concat([df_master, df_distinct], axis=0)\n",
    "\n",
    "        #drop the distinct vgps from the selected list\n",
    "        df.drop(df_distinct.index, axis=0, inplace=True)\n",
    "    \n",
    "    return (df, df_master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2febf231-38d7-4dac-933c-4a5d888eff48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vgps, df_vgps_master = strip_age_distinct(df_vgps, df_vgps_master, criteria_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb98862-df94-4bee-af19-689c9eb9db15",
   "metadata": {},
   "source": [
    "## Compute provisional paleopole\n",
    "Now we can compute a provisional paleopole and check for anomalous vgps (according to the definition of 'anomalous' as specified in the inclusion criteria). To rigorously compute a pole presents something of a headache because we should first pre-average any common-time units, but some of these could potentially include anomalous directions that would susequently be dismissed, etc. Here we adopt a simpler approach: defaulting to the subset of selected data which the original authors also retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "72164296-3ea7-4773-a054-83c06b9618db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_anomalous (df, criteria_codes):\n",
    "\n",
    "    if 9 in criteria_codes: \n",
    "        \n",
    "        #isolate the entries to be used for provisional paleopole calculation\n",
    "        df_prov = df[(df['in_study_pole'] != 0) & (df['keep'] == True)]\n",
    "\n",
    "        #calculate provisional paleopole\n",
    "        ppole = ipmag.fisher_mean(dec = df_prov['VGP_lon_Rpol'].tolist(), inc = df_prov['VGP_lat_Rpol'].tolist())\n",
    "        print ('Provisional pole: ', ppole)\n",
    "\n",
    "        #identify anomalous vgps according to the specification above\n",
    "        df['keep'] = df.apply(lambda row: False if (pmag.angle([row.VGP_lon_Rpol, row.VGP_lat_Rpol], [ppole['dec'], ppole['inc']]) \\\n",
    "                                                              > incl_criteria['anomalous_dir']) else row.keep, axis=1)\n",
    "\n",
    "        #drop anomalous entries\n",
    "        df.drop(df[df.keep == False].index, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0449dc83-6c15-4b31-bff2-63ed91825438",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vgps = strip_anomalous(df_vgps, criteria_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77913401-1144-4895-afb2-8f4243e3b860",
   "metadata": {},
   "source": [
    "### Remove sparse time_units collections\n",
    "After filtering there may be some common time_unit collections with too few entries to be useful to pass onward to the master dataframe. In these cases, we can adopt any existing reported mean and discard the individual entries. In other cases, where there exists a sufficient number of individual entries we can pass them on and omit the reported mean. For this we need to decide what is 'too few': we default to either the minimum number of samples (n) as specified above or <= 3 (whichever is higher), but this can otherwise be adjusted to any other arbitrary value larger than 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "352468ec-cedb-4e84-a71f-804762b482f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_sparse_time_units (df, incl_criteria):\n",
    "\n",
    "    # first specify an n-specific cutoff value to decide whether to recalculate mean or retain the reported one.\n",
    "    min_site_count = 3\n",
    "    if incl_criteria['sample_count'] != None and incl_criteria['sample_count'].isdigit():\n",
    "        if incl_criteria['sampl_count'] > min_site_count: min_site_count = int(incl_criteria['sampl_count'])\n",
    "            \n",
    "    # now extract common time units from selected list and split into groups\n",
    "    df['keep'] = df.apply(lambda row: False if row.time_unit != '0' else row.keep, axis=1)\n",
    "    df_redundant = df[df['time_unit'] != '0']\n",
    "    time_units = df_redundant.groupby(['time_unit'])\n",
    "    \n",
    "    # collect the means into a list to make them easy to find when needed\n",
    "    means = [x for x in time_units.groups if x.isdigit() == False]\n",
    "\n",
    "    # now check the number of sites for each given group\n",
    "    for key, group in time_units:\n",
    "        if key.isdigit():    # ignore means\n",
    "            \n",
    "            if len(group) > min_site_count:   # if number of sites is sufficient, keep all individual sites\n",
    "                df.loc[group.index.tolist(), 'keep'] = True\n",
    "\n",
    "            elif ('M'+str(key)) in means:     # if number of sites is too low and mean is reported, keep mean\n",
    "                mean_ent = time_units.get_group('M'+str(key))\n",
    "                df.loc[mean_ent.index.tolist(), 'keep'] = True\n",
    "                df.loc[mean_ent.index.tolist(), 'time_unit'] = '0' # set time_unit to 0 as there is now only 1 entry\n",
    "\n",
    "            else:      # if number of sites is too low and no mean is reported, use site with smaller alpha95 (or higher n)\n",
    "                df.loc[group['alpha95'].idxmin(), 'keep'] = True\n",
    "                ### alternatively: group['n'].idxmax()\n",
    "                df.loc[group['alpha95'].idxmin(), 'time_unit'] = '0'  # set time_unit to 0 as there is now only 1 entry\n",
    "                \n",
    "    #drop discarded entries\n",
    "    df.drop(df[df.keep == False].index, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "76d67696-8094-4526-b995-2f507f0fe8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vgps = strip_sparse_time_units(df_vgps, incl_criteria)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cbcb02-c3fd-455f-9bdd-0544623991fe",
   "metadata": {},
   "source": [
    "## Append filtered data to master dataframe\n",
    "Pass the final selected entries to the master vgp dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b473666d-1050-4605-a6d4-f689ada6c281",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vgps_master = pd.concat([df_vgps_master, df_vgps], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fedeb9-a5d5-4dcb-8b2a-88f513825cbd",
   "metadata": {},
   "source": [
    "## Final paleopole recalculation\n",
    "Finally, we can recalculate the paleopole based on only the filtered set of vgp data. First we need to pre-average the common time_units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6a33374e-b750-4c06-896f-3367cef66115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_time_units (df): # pre-average the common time units\n",
    "    \n",
    "    df['keep'] = df.apply(lambda row: False if row.time_unit != '0' else row.keep, axis=1)\n",
    "    df_redundant = df[df['time_unit'] != '0']\n",
    "    time_units = df_redundant.groupby(['time_unit'])\n",
    "    \n",
    "    for key, group in time_units:\n",
    "        mean_age = group['mean_age'].mean(axis=0) # get mean age from among time_unit sites *** NOTE THIS DOESN'T COLLECT / PASS ON UNCERTAINTIES ***\n",
    "        \n",
    "        vgp = ipmag.fisher_mean(dec = group['VGP_lon_Rpol'].tolist(), inc = group['VGP_lat_Rpol'].tolist()) # compute vgp from among time_units\n",
    "        \n",
    "        df.append({'VGP_lon': vgp['dec'], 'VGP_lon_Rpol': vgp['dec'], 'VGP_lat': vgp['inc'], 'VGP_lat_Rpol': vgp['inc'],\n",
    "                   'A95': vgp['alpha95'], 'mean_age': mean_age, 'keep': True}, ignore_index=True)    # other entries could be added but aren't presently needed\n",
    "        \n",
    "    #drop discarded entries\n",
    "    df.drop(df[df.keep == False].index, inplace=True)\n",
    "    \n",
    "    return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b12bd674-3554-4d70-b076-5866c525f764",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vgps = average_time_units(df_vgps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93aca8e1-ccd8-494a-b7f1-51fcddff31b7",
   "metadata": {},
   "source": [
    "Now compute the final paleopole, determine its corresponding age, and append it to the re-calculated paleopole dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5efdd2bb-5a3b-4dff-b5ff-fb48503cfe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pole (df):\n",
    "    \n",
    "    mean_age = df['mean_age'].mean(axis=0) # get mean pole age  *** NOTE THIS DOESN'T COLLECT / PASS ON UNCERTAINTIES ***\n",
    "    \n",
    "    paleopole = ipmag.fisher_mean(dec = df['VGP_lon_Rpol'].tolist(), inc = df['VGP_lat_Rpol'].tolist())\n",
    "    \n",
    "    print (mean_age, paleopole['dec'], paleopole['inc'])\n",
    "    \n",
    "# df_recalculated_poles_master = pd.concat([df_recalc_pps_master, recalc_pp], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3fe515c1-fd06-4aaa-8015-556c4c163bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.115694444444444 82.75842518665542 -88.3440171117142\n"
     ]
    }
   ],
   "source": [
    "df_vgps = compute_pole(df_vgps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f81855f-ad92-49e3-865d-c49766980e65",
   "metadata": {},
   "source": [
    "## Execute on the entire dataframe\n",
    "Now that we have demonstrated the workflow on an example datasheet, we can execute it across the entire dataset by cycling through all the datasheets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cdb3a50f-9900-46c8-8128-e70037edb0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no missing site information\n",
      "vgp from site pwrLIB in file index 0 appears to be inverted. Flipping back.\n",
      "vgp from site pwnPAL in file index 0 appears to be inverted. Flipping back.\n",
      "vgp from site pwrJAL in file index 0 appears to be inverted. Flipping back.\n",
      "vgp from site pwrSJG in file index 0 appears to be inverted. Flipping back.\n",
      "vgp from site pwrFER in file index 0 appears to be inverted. Flipping back.\n",
      "vgp from site mcrARE in file index 0 appears to be inverted. Flipping back.\n",
      "vgp from site pcrCHA in file index 0 appears to be inverted. Flipping back.\n",
      "vgp from site mcrFIN in file index 0 appears to be inverted. Flipping back.\n",
      "vgp from site pcrTRO in file index 0 appears to be inverted. Flipping back.\n",
      "vgp from site pcrOCO in file index 0 appears to be inverted. Flipping back.\n",
      "vgp from site qcrCG in file index 0 appears to be inverted. Flipping back.\n",
      "vgp from site pcrPEN in file index 0 appears to be inverted. Flipping back.\n",
      "vgp from site qcrEST in file index 0 appears to be inverted. Flipping back.\n",
      "vgp from site qcrROD in file index 0 appears to be inverted. Flipping back.\n",
      "vgp from site qcrCSA in file index 0 appears to be inverted. Flipping back.\n",
      "vgp from site pcrHUA in file index 0 appears to be inverted. Flipping back.\n",
      "vgp from site pcrSOL in file index 0 appears to be inverted. Flipping back.\n",
      "vgp from site mcrCAN in file index 0 appears to be inverted. Flipping back.\n",
      "vgp from site mcrORD in file index 0 appears to be inverted. Flipping back.\n",
      "vgp from site mcrAZU in file index 0 appears to be inverted. Flipping back.\n",
      "***SPURIOUS*** vgp from site pcrTRA in file index 0; reported pole differs from re-calculated by 173 degrees. CHECK against original reference\n",
      "no missing alpha95 information\n",
      "4.498023255813953 101.01247914328152 -87.85046136543447\n",
      "Missing slat/slon and/or dec/inc at site pwrIXT in file index 1 where no vgp is reported; cannot calculate vgp -- dropping entry\n",
      "no missing site information\n",
      "vgp from site pwrLIB in file index 1 appears to be inverted. Flipping back.\n",
      "vgp from site pwnPAL in file index 1 appears to be inverted. Flipping back.\n",
      "vgp from site pwrJAL in file index 1 appears to be inverted. Flipping back.\n",
      "vgp from site pwrSJG in file index 1 appears to be inverted. Flipping back.\n",
      "vgp from site pwrFER in file index 1 appears to be inverted. Flipping back.\n",
      "vgp from site mcrARE in file index 1 appears to be inverted. Flipping back.\n",
      "vgp from site pcrCHA in file index 1 appears to be inverted. Flipping back.\n",
      "vgp from site mcrFIN in file index 1 appears to be inverted. Flipping back.\n",
      "vgp from site pcrTRO in file index 1 appears to be inverted. Flipping back.\n",
      "vgp from site pcrOCO in file index 1 appears to be inverted. Flipping back.\n",
      "vgp from site qcrCG in file index 1 appears to be inverted. Flipping back.\n",
      "vgp from site pcrPEN in file index 1 appears to be inverted. Flipping back.\n",
      "vgp from site qcrEST in file index 1 appears to be inverted. Flipping back.\n",
      "vgp from site qcrROD in file index 1 appears to be inverted. Flipping back.\n",
      "vgp from site qcrCSA in file index 1 appears to be inverted. Flipping back.\n",
      "vgp from site pcrHUA in file index 1 appears to be inverted. Flipping back.\n",
      "vgp from site pcrSOL in file index 1 appears to be inverted. Flipping back.\n",
      "vgp from site mcrCAN in file index 1 appears to be inverted. Flipping back.\n",
      "vgp from site mcrORD in file index 1 appears to be inverted. Flipping back.\n",
      "vgp from site mcrAZU in file index 1 appears to be inverted. Flipping back.\n",
      "***SPURIOUS*** vgp from site pcrTRA in file index 1; reported pole differs from re-calculated by 173 degrees. CHECK against original reference\n",
      "no missing alpha95 information\n",
      "4.115694444444444 82.75842518665542 -88.3440171117142\n"
     ]
    }
   ],
   "source": [
    "for i in files.index:   # cycle over each file in database\n",
    "    \n",
    "    df_poles, df_vgps = split_datasheet(files, i)\n",
    "    df_vgps = get_vgps(df_vgps, i)\n",
    "    df_vgps = get_sites(df_vgps, i)\n",
    "    df_vgps = check_vgps (df_vgps, i)\n",
    "    df_vgps = get_common_polarity(df_vgps)\n",
    "    df_vgps = get_alpha95s(df_vgps, i)\n",
    "    df_vgps = get_ages(df_vgps)\n",
    "    df_vgps, strat_group_counter, time_unit_counter = assign_uniq_ids(df_vgps, strat_group_counter, time_unit_counter)\n",
    "    df_vgps = init_filter(df_vgps, incl_criteria, criteria_codes)\n",
    "    df_vgps, df_vgps_master = strip_age_distinct(df_vgps, df_vgps_master, criteria_codes)\n",
    "    df_vgps = strip_anomalous(df_vgps, criteria_codes)\n",
    "    df_vgps = strip_sparse_time_units(df_vgps, incl_criteria)\n",
    "    df_vgps_master = pd.concat([df_vgps_master, df_vgps], axis=0)\n",
    "    \n",
    "    df_vgps = average_time_units(df_vgps)\n",
    "    df_vgps = compute_pole(df_vgps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "360283a9-8d20-497b-847c-b2706b83d0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Other things to do:\n",
    "### - implement capacity to eject results from poles that are known to be regionally rotated (this requires additional fields being added to the database)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
